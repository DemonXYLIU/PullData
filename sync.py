import pymysql
import subprocess
import sys
import os
import json
import threading
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed

# ================= é…ç½®åŒºåŸŸ =================
DATAX_PATH = "/Users/demon/Downloads/datax/bin/datax.py"
CHECKPOINT_FILE = "checkpoint.json"
MAX_WORKERS = 8  # å¹¶å‘çº¿ç¨‹æ•°

# æºæ•°æ®åº“
SRC_CONFIG = {
    'host': '10.11.252.103', 'user': 'root', 'password': 'gY~~2Vqi2-DQ',
    'db': 'meicloud_plm', 'port': 33306, 'charset': 'utf8'
}

# æœ¬åœ°æ•°æ®åº“
DEST_CONFIG = {
    'host': 'localhost', 'user': 'root', 'password': '123456',
    'db': 'meicloud_plm', 'port': 3306, 'charset': 'utf8'
}
# ===========================================

file_lock = threading.Lock()

def get_connection(config):
    return pymysql.connect(
        host=config['host'], user=config['user'], password=config['password'],
        db=config['db'], port=config['port'], charset=config['charset']
    )

def load_checkpoint():
    if not os.path.exists(CHECKPOINT_FILE):
        return {}
    with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:
        try:
            return json.load(f)
        except:
            return {}

def update_checkpoint(table, time_str):
    with file_lock:
        data = load_checkpoint()
        data[table] = time_str
        with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=4, ensure_ascii=False)

def get_local_max_time(conn, table):
    try:
        with conn.cursor() as cursor:
            cursor.execute(f"SHOW TABLES LIKE '{table}'")
            if not cursor.fetchone(): return None
            cursor.execute(f"SHOW COLUMNS FROM `{table}` LIKE 'editTime'")
            if not cursor.fetchone(): return None
            cursor.execute(f"SELECT MAX(editTime) FROM `{table}`")
            res = cursor.fetchone()
            return str(res[0]) if res and res[0] else None
    except:
        return None

def get_table_columns_quoted(conn, db, table):
    """
    è·å–è¡¨çš„æ‰€æœ‰å­—æ®µ,å¹¶ç»™æ¯ä¸ªå­—æ®µåŠ ä¸Šåå¼•å· `field`
    """
    sql = f"""
    SELECT COLUMN_NAME 
    FROM information_schema.COLUMNS 
    WHERE TABLE_SCHEMA = '{db}' AND TABLE_NAME = '{table}' 
    ORDER BY ORDINAL_POSITION
    """
    with conn.cursor() as cursor:
        cursor.execute(sql)
        # å°†æ¯ä¸ªå­—æ®µåç”¨åå¼•å·åŒ…èµ·æ¥,è§£å†³ KEY, VALUE, CONDITION ç­‰å…³é”®å­—æŠ¥é”™é—®é¢˜
        return [f"`{row[0]}`" for row in cursor.fetchall()]

def get_primary_keys(conn, db, table):
    """
    è·å–è¡¨çš„ä¸»é”®å­—æ®µåˆ—è¡¨
    è¿”å›: ä¸»é”®å­—æ®µååˆ—è¡¨,å¦‚æœæ²¡æœ‰ä¸»é”®è¿”å›ç©ºåˆ—è¡¨
    """
    sql = f"""
    SELECT COLUMN_NAME
    FROM information_schema.KEY_COLUMN_USAGE
    WHERE TABLE_SCHEMA = '{db}' 
    AND TABLE_NAME = '{table}'
    AND CONSTRAINT_NAME = 'PRIMARY'
    ORDER BY ORDINAL_POSITION
    """
    with conn.cursor() as cursor:
        cursor.execute(sql)
        return [row[0] for row in cursor.fetchall()]

def detect_and_delete_orphaned_records(src_conn, dest_conn, table, pk_fields, db_config):
    """
    æ£€æµ‹å¹¶åˆ é™¤ç›®æ ‡è¡¨ä¸­å¤šä½™çš„è®°å½•(æºè¡¨å·²åˆ é™¤ä½†ç›®æ ‡è¡¨ä»å­˜åœ¨çš„è®°å½•)
    
    å‚æ•°:
        src_conn: æºæ•°æ®åº“è¿æ¥
        dest_conn: ç›®æ ‡æ•°æ®åº“è¿æ¥
        table: è¡¨å
        pk_fields: ä¸»é”®å­—æ®µåˆ—è¡¨
        db_config: æ•°æ®åº“é…ç½®(ç”¨äºè·å–æ•°æ®åº“å)
    
    è¿”å›:
        åˆ é™¤çš„è®°å½•æ•°
    """
    if not pk_fields:
        return 0
    
    try:
        # æ„å»ºä¸»é”®å­—æ®µçš„ SQL ç‰‡æ®µ
        pk_columns = ', '.join([f"`{pk}`" for pk in pk_fields])
        
        # è·å–æºè¡¨çš„æ‰€æœ‰ä¸»é”®å€¼
        src_cursor = src_conn.cursor()
        src_cursor.execute(f"SELECT {pk_columns} FROM `{table}`")
        src_pks = set(src_cursor.fetchall())
        src_cursor.close()
        
        # è·å–ç›®æ ‡è¡¨çš„æ‰€æœ‰ä¸»é”®å€¼
        dest_cursor = dest_conn.cursor()
        dest_cursor.execute(f"SELECT {pk_columns} FROM `{table}`")
        dest_pks = set(dest_cursor.fetchall())
        dest_cursor.close()
        
        # è®¡ç®—éœ€è¦åˆ é™¤çš„è®°å½•(ç›®æ ‡è¡¨æœ‰ä½†æºè¡¨æ²¡æœ‰)
        orphaned_pks = dest_pks - src_pks
        
        if not orphaned_pks:
            return 0
        
        # åˆ é™¤å¤šä½™çš„è®°å½•
        deleted_count = 0
        dest_cursor = dest_conn.cursor()
        
        for pk_values in orphaned_pks:
            # æ„å»º WHERE æ¡ä»¶
            if len(pk_fields) == 1:
                # å•ä¸»é”®
                where_clause = f"`{pk_fields[0]}` = %s"
                dest_cursor.execute(f"DELETE FROM `{table}` WHERE {where_clause}", (pk_values,))
            else:
                # å¤åˆä¸»é”®
                conditions = [f"`{pk}` = %s" for pk in pk_fields]
                where_clause = ' AND '.join(conditions)
                dest_cursor.execute(f"DELETE FROM `{table}` WHERE {where_clause}", pk_values)
            
            deleted_count += dest_cursor.rowcount
        
        dest_conn.commit()
        dest_cursor.close()
        
        return deleted_count
        
    except Exception as e:
        print(f"    âš ï¸  åˆ é™¤æ£€æµ‹å¤±è´¥: {str(e)}")
        return 0

def process_table(table, force_full_sync=False, detect_deletes=True, truncate_before_sync=False):
    try:
        src_conn = get_connection(SRC_CONFIG)
        dest_conn = get_connection(DEST_CONFIG)
    except Exception as e:
        return f"âŒ {table}: æ•°æ®åº“è¿æ¥å¤±è´¥ - {str(e)}"

    # åŠ¨æ€ç”Ÿæˆ JSON æ–‡ä»¶çš„è·¯å¾„
    temp_json_file = f"tmp_job_{table}.json"
    result_msg = ""
    
    try:
        # 1. è·å–å¸¦æœ‰åå¼•å·çš„å­—æ®µåˆ—è¡¨ (å…³é”®æ­¥éª¤ï¼)
        columns_quoted = get_table_columns_quoted(src_conn, SRC_CONFIG['db'], table)
        if not columns_quoted:
            return f"âŒ {table}: æ— æ³•è·å–å­—æ®µä¿¡æ¯ï¼Œè·³è¿‡"

        # 2. æ£€æŸ¥æ˜¯å¦æœ‰ editTime
        with src_conn.cursor() as cursor:
            cursor.execute(f"""
                SELECT count(*) FROM information_schema.COLUMNS 
                WHERE TABLE_SCHEMA = '{SRC_CONFIG['db']}' 
                AND TABLE_NAME = '{table}' 
                AND COLUMN_NAME = 'editTime'
            """)
            has_edittime = cursor.fetchone()[0] > 0

        where_clause = ""
        current_max_time = None
        is_incremental = False
        checkpoints = load_checkpoint()

        # å¼ºåˆ¶å…¨é‡åŒæ­¥æ¨¡å¼
        if force_full_sync:
            where_clause = "1=1"
            result_msg = f"ğŸ”„ {table}: å¼ºåˆ¶å…¨é‡åŒæ­¥"
            # å¦‚æœæœ‰ editTime,è·å–å½“å‰æœ€å¤§æ—¶é—´ç”¨äºæ›´æ–° checkpoint
            if has_edittime:
                with src_conn.cursor() as cursor:
                    cursor.execute(f"SELECT MAX(editTime) FROM `{table}`")
                    res = cursor.fetchone()[0]
                    if res:
                        current_max_time = str(res)
        elif not has_edittime:
            where_clause = "1=1"
            result_msg = f"ğŸ”„ {table}: å…¨é‡åŒæ­¥ (æ—  editTime)"
        else:
            start_time = "1970-01-01 00:00:00"
            if table in checkpoints:
                start_time = checkpoints[table]
            else:
                local_max = get_local_max_time(dest_conn, table)
                if local_max: start_time = local_max

            with src_conn.cursor() as cursor:
                cursor.execute(f"SELECT MAX(editTime) FROM `{table}`")
                res = cursor.fetchone()[0]
                if res is None:
                    return f"âš ï¸ {table}: æºè¡¨ä¸ºç©ºï¼Œè·³è¿‡"
                current_max_time = str(res)

            if current_max_time <= start_time:
                return f"â¹ï¸  {table}: æ— æ–°æ•°æ® (Current: {current_max_time})"

            where_clause = f"editTime > '{start_time}' AND editTime <= '{current_max_time}'"
            is_incremental = True
            result_msg = f"ğŸš€ {table}: å¢é‡åŒæ­¥ ({start_time} -> {current_max_time})"

        # 3. åŠ¨æ€æ„å»º DataX JSON é…ç½®å­—å…¸
        # æˆ‘ä»¬ä¸å†è¯»å– job.json æ¨¡æ¿ï¼Œè€Œæ˜¯ç›´æ¥åœ¨å†…å­˜é‡Œç”Ÿæˆé…ç½®
        # è¿™æ ·å¯ä»¥å°† columns_quoted åˆ—è¡¨å®Œç¾åµŒå…¥ï¼Œä¸ä¼šæœ‰æ ¼å¼é—®é¢˜
        job_config = {
            "job": {
                "content": [{
                    "reader": {
                        "name": "mysqlreader",
                        "parameter": {
                            "username": SRC_CONFIG['user'],
                            "password": SRC_CONFIG['password'],
                            "column": columns_quoted,  # ä½¿ç”¨å¸¦åå¼•å·çš„å­—æ®µåˆ—è¡¨
                            "connection": [{
                                "jdbcUrl": [f"jdbc:mysql://{SRC_CONFIG['host']}:{SRC_CONFIG['port']}/{SRC_CONFIG['db']}?useUnicode=true&characterEncoding=utf8"],
                                "table": [table]
                            }],
                            "where": where_clause
                        }
                    },
                    "writer": {
                        "name": "mysqlwriter",
                        "parameter": {
                            "username": DEST_CONFIG['user'],
                            "password": DEST_CONFIG['password'],
                            "writeMode": "replace",
                            "column": columns_quoted,  # å†™å…¥ç«¯ä¹Ÿç”¨åŒæ ·çš„å­—æ®µåˆ—è¡¨
                            "connection": [{
                                "jdbcUrl": f"jdbc:mysql://{DEST_CONFIG['host']}:{DEST_CONFIG['port']}/{DEST_CONFIG['db']}?useUnicode=true&characterEncoding=utf8&rewriteBatchedStatements=true",
                                "table": [table]
                            }]
                        }
                    }
                }],
                "setting": {
                    "speed": {"channel": 5}
                }
            }
        }

        # 4. å°†é…ç½®å†™å…¥ä¸´æ—¶ JSON æ–‡ä»¶
        with open(temp_json_file, 'w', encoding='utf-8') as f:
            json.dump(job_config, f, ensure_ascii=False)

        # 5. å…¨é‡åŒæ­¥æ¨¡å¼:å¯é€‰æ‹©å…ˆæ¸…ç©ºç›®æ ‡è¡¨
        if force_full_sync and truncate_before_sync:
            try:
                with dest_conn.cursor() as cursor:
                    cursor.execute(f"TRUNCATE TABLE `{table}`")
                dest_conn.commit()
                result_msg += " (å·²æ¸…ç©ºç›®æ ‡è¡¨)"
            except Exception as e:
                print(f"    âš ï¸  æ¸…ç©ºè¡¨å¤±è´¥: {str(e)}")
        
        # 6. è°ƒç”¨ DataX (ç›´æ¥æŒ‡å‘ä¸´æ—¶æ–‡ä»¶,ä¸éœ€è¦ -p å‚æ•°äº†)
        cmd = ["python3", DATAX_PATH, temp_json_file]

        result = subprocess.run(
            cmd, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.STDOUT,
            encoding='utf-8',       
            errors='ignore'
        )
        
        # 7. åˆ é™¤ä¸´æ—¶é…ç½®æ–‡ä»¶ (æ¸…ç†ç°åœº)
        if os.path.exists(temp_json_file):
            os.remove(temp_json_file)

        if result.returncode != 0:
            log_file = f"error_{table}.log"
            with open(log_file, "w", encoding='utf-8') as f:
                f.write(result.stdout)
            
            # æå–é”™è¯¯æ‘˜è¦
            log_lines = result.stdout.splitlines()
            error_summary = [line.strip() for line in log_lines if "Exception" in line or "Error" in line]
            if not error_summary: error_summary = log_lines[-5:]
            summary_str = "\n    ".join(error_summary[-2:]) 

            return f"âŒ {table} å¤±è´¥!(æ—¥å¿—: {log_file})\n    åŸå› : {summary_str}"

        # 8. åˆ é™¤æ£€æµ‹:æ£€æµ‹å¹¶åˆ é™¤ç›®æ ‡è¡¨ä¸­å¤šä½™çš„è®°å½•
        deleted_count = 0
        if detect_deletes and not (force_full_sync and truncate_before_sync):
            # å¦‚æœæ˜¯å…¨é‡åŒæ­¥ä¸”å·²æ¸…ç©ºè¡¨,åˆ™ä¸éœ€è¦åˆ é™¤æ£€æµ‹
            try:
                pk_fields = get_primary_keys(src_conn, SRC_CONFIG['db'], table)
                if pk_fields:
                    deleted_count = detect_and_delete_orphaned_records(
                        src_conn, dest_conn, table, pk_fields, SRC_CONFIG
                    )
                    if deleted_count > 0:
                        result_msg += f" (åˆ é™¤ {deleted_count} æ¡)"
                else:
                    # æ²¡æœ‰ä¸»é”®,è·³è¿‡åˆ é™¤æ£€æµ‹
                    if detect_deletes:
                        result_msg += " (æ— ä¸»é”®,è·³è¿‡åˆ é™¤æ£€æµ‹)"
            except Exception as e:
                result_msg += f" (åˆ é™¤æ£€æµ‹å¼‚å¸¸: {str(e)})"
        
        # 9. æ›´æ–° checkpoint: å¢é‡åŒæ­¥æˆ–å¼ºåˆ¶å…¨é‡åŒæ­¥(æœ‰ editTime)
        if current_max_time and (is_incremental or force_full_sync):
            update_checkpoint(table, current_max_time)
            
        return result_msg + " [âœ… æˆåŠŸ]"

    except Exception as e:
        return f"âŒ {table}: è„šæœ¬å¼‚å¸¸ - {str(e)}"
    finally:
        try:
            src_conn.close()
            dest_conn.close()
        except:
            pass

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='MySQL æ•°æ®åŒæ­¥å·¥å…· - æ”¯æŒå¢é‡å’Œå…¨é‡åŒæ­¥',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ä½¿ç”¨ç¤ºä¾‹:
  # åŒæ­¥æ‰€æœ‰è¡¨(å¢é‡æ¨¡å¼)
  python3 sync.py
  
  # å¯¹æ‰€æœ‰è¡¨è¿›è¡Œå…¨é‡åŒæ­¥
  python3 sync.py --full
  
  # æ’é™¤æŸäº›è¡¨,åŒæ­¥å…¶ä»–æ‰€æœ‰è¡¨
  python3 sync.py --exclude sys_log sys_temp
  
  # å¯¹æŒ‡å®šè¡¨è¿›è¡Œå…¨é‡åŒæ­¥
  python3 sync.py --tables table1 table2 table3 --full
  
  # å¯¹æŒ‡å®šè¡¨è¿›è¡Œå¢é‡åŒæ­¥
  python3 sync.py --tables table1 table2
  
  # å…¨é‡åŒæ­¥æ‰€æœ‰è¡¨,ä½†æ’é™¤æŸäº›è¡¨
  python3 sync.py --full --exclude sys_log sys_temp
  
  # ç¦ç”¨åˆ é™¤æ£€æµ‹(é»˜è®¤å¯ç”¨)
  python3 sync.py --no-detect-deletes
  
  # å…¨é‡åŒæ­¥å‰æ¸…ç©ºè¡¨
  python3 sync.py --full --truncate-before-sync
        '''
    )
    
    parser.add_argument(
        '--tables', '-t',
        nargs='+',
        metavar='TABLE',
        help='æŒ‡å®šè¦åŒæ­¥çš„è¡¨å(æ”¯æŒå¤šä¸ªè¡¨,ç”¨ç©ºæ ¼åˆ†éš”)'
    )
    
    parser.add_argument(
        '--exclude', '-e',
        nargs='+',
        metavar='TABLE',
        help='æŒ‡å®šè¦æ’é™¤çš„è¡¨å(æ”¯æŒå¤šä¸ªè¡¨,ç”¨ç©ºæ ¼åˆ†éš”)'
    )
    
    parser.add_argument(
        '--full', '-f',
        action='store_true',
        help='å¼ºåˆ¶å…¨é‡åŒæ­¥(å¿½ç•¥ checkpoint å’Œ editTime)'
    )
    
    parser.add_argument(
        '--no-detect-deletes',
        action='store_true',
        help='ç¦ç”¨åˆ é™¤æ£€æµ‹(é»˜è®¤å¯ç”¨åˆ é™¤æ£€æµ‹ä»¥ä¿è¯æ•°æ®ä¸€è‡´æ€§)'
    )
    
    parser.add_argument(
        '--truncate-before-sync',
        action='store_true',
        help='å…¨é‡åŒæ­¥å‰æ¸…ç©ºç›®æ ‡è¡¨(ä»…åœ¨ --full æ¨¡å¼ä¸‹ç”Ÿæ•ˆ)'
    )
    
    args = parser.parse_args()
    
    if not os.path.exists(DATAX_PATH):
        print(f"âŒ DataX è·¯å¾„é”™è¯¯: {DATAX_PATH}")
        return

    # è·å–è¦å¤„ç†çš„è¡¨åˆ—è¡¨
    if args.tables:
        # ç”¨æˆ·æŒ‡å®šäº†è¡¨å
        tables = args.tables
        print(f"ğŸ“‹ ç”¨æˆ·æŒ‡å®š {len(tables)} å¼ è¡¨: {', '.join(tables)}")
        
        # éªŒè¯è¡¨æ˜¯å¦å­˜åœ¨
        try:
            conn = get_connection(SRC_CONFIG)
            with conn.cursor() as cursor:
                cursor.execute("SHOW TABLES")
                all_tables = {row[0] for row in cursor.fetchall()}
            conn.close()
            
            # æ£€æŸ¥ä¸å­˜åœ¨çš„è¡¨
            invalid_tables = [t for t in tables if t not in all_tables]
            if invalid_tables:
                print(f"âš ï¸  è­¦å‘Š: ä»¥ä¸‹è¡¨ä¸å­˜åœ¨: {', '.join(invalid_tables)}")
                tables = [t for t in tables if t in all_tables]
                if not tables:
                    print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è¡¨å¯ä»¥åŒæ­¥")
                    return
                print(f"âœ… å°†åŒæ­¥ä»¥ä¸‹æœ‰æ•ˆè¡¨: {', '.join(tables)}")
        except Exception as e:
            print(f"âŒ è¿æ¥æºåº“å¤±è´¥: {e}")
            return
    else:
        # åŒæ­¥æ‰€æœ‰è¡¨
        print("æ­£åœ¨è·å–è¡¨æ¸…å•...")
        try:
            conn = get_connection(SRC_CONFIG)
            with conn.cursor() as cursor:
                cursor.execute("SHOW TABLES")
                tables = [row[0] for row in cursor.fetchall()]
            conn.close()
        except Exception as e:
            print(f"âŒ è¿æ¥æºåº“å¤±è´¥: {e}")
            return
        
        print(f"ğŸ“‹ å‘ç° {len(tables)} å¼ è¡¨")
    
    # å¤„ç†æ’é™¤è¡¨é€»è¾‘
    if args.exclude:
        excluded_count = 0
        original_count = len(tables)
        excluded_tables = []
        
        for exclude_table in args.exclude:
            if exclude_table in tables:
                tables.remove(exclude_table)
                excluded_tables.append(exclude_table)
                excluded_count += 1
        
        if excluded_count > 0:
            print(f"ğŸš« æ’é™¤ {excluded_count} å¼ è¡¨: {', '.join(excluded_tables)}")
            print(f"âœ… å®é™…åŒæ­¥ {len(tables)} å¼ è¡¨ (åŸ {original_count} å¼ )")
        else:
            print(f"âš ï¸  è­¦å‘Š: æŒ‡å®šçš„æ’é™¤è¡¨ä¸å­˜åœ¨: {', '.join(args.exclude)}")
        
        if not tables:
            print("âŒ æ²¡æœ‰è¡¨éœ€è¦åŒæ­¥")
            return
    
    # æ˜¾ç¤ºåŒæ­¥æ¨¡å¼
    sync_mode = "å¼ºåˆ¶å…¨é‡åŒæ­¥" if args.full else "æ™ºèƒ½åŒæ­¥(å¢é‡/å…¨é‡)"
    detect_deletes = not args.no_detect_deletes  # é»˜è®¤å¯ç”¨åˆ é™¤æ£€æµ‹
    
    print(f"ğŸ”§ åŒæ­¥æ¨¡å¼: {sync_mode}")
    print(f"ğŸ” åˆ é™¤æ£€æµ‹: {'å¯ç”¨' if detect_deletes else 'ç¦ç”¨'}")
    if args.truncate_before_sync and args.full:
        print(f"ğŸ—‘ï¸  æ¸…ç©ºè¡¨æ¨¡å¼: å¯ç”¨(å…¨é‡åŒæ­¥å‰æ¸…ç©ºè¡¨)")
    print(f"âš™ï¸  å¹¶å‘çº¿ç¨‹æ•°: {MAX_WORKERS}")
    print("=" * 60)

    # æ‰§è¡ŒåŒæ­¥
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # æ ¹æ®æ˜¯å¦å¼ºåˆ¶å…¨é‡åŒæ­¥æ¥æäº¤ä»»åŠ¡
        if args.full:
            future_to_table = {
                executor.submit(
                    process_table, 
                    table, 
                    force_full_sync=True,
                    detect_deletes=detect_deletes,
                    truncate_before_sync=args.truncate_before_sync
                ): table 
                for table in tables
            }
        else:
            future_to_table = {
                executor.submit(
                    process_table, 
                    table,
                    detect_deletes=detect_deletes
                ): table 
                for table in tables
            }
        
        for future in as_completed(future_to_table):
            table = future_to_table[future]
            try:
                msg = future.result()
                if "â¹ï¸" not in msg: 
                    print(msg)
            except Exception as exc:
                print(f"âŒ {table} çº¿ç¨‹å¼‚å¸¸: {exc}")

    print("=" * 60)
    print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡ç»“æŸã€‚")

if __name__ == "__main__":
    main()